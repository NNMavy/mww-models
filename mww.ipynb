{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e8fc0db",
      "metadata": {
        "id": "8e8fc0db"
      },
      "source": [
        "# microWakeWord\n",
        "This notebook is based on basic_training_notebook.ipynb from microWakeWord from here: https://github.com/kahrendt/microWakeWord/blob/main/notebooks/basic_training_notebook.ipynb.\n",
        "\n",
        "And hausklaus from here: https://github.com/ABee81/hausklaus-mww/blob/main/hausklaus-mww.ipynb\n",
        "\n",
        "Datasets were extended, configured properly for mls model and training parameters were changed for longer training.\n",
        "Executing this notebook step by step will create a stream_state_internal_quant.tflite file in data/trained_models/mww/tflite_stream_state_internal_quant.\n",
        "This model can then be used in esphome on the HA VPE for an on device wakeword detection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d11a33",
      "metadata": {
        "id": "d0d11a33"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download all prerequirements\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"data/downloads\"):\n",
        "    os.mkdir(\"./data\")\n",
        "    os.mkdir(\"./data/downloads\")\n",
        "\n",
        "for i in range(10):\n",
        "    fname = f\"bal_train0{i}.tar\"\n",
        "    out_path = f\"data/downloads/{fname}\"\n",
        "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
        "    if not os.path.exists(f\"{out_path}\"):\n",
        "      !wget -O {out_path} {link}\n",
        "\n",
        "if not os.path.exists(\"data/downloads/fma_small.zip\"):\n",
        "  !wget -O data/downloads/fma_small.zip https://os.unil.cloud.switch.ch/fma/fma_small.zip\n",
        "\n",
        "for fname in ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']:\n",
        "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
        "\n",
        "    link = link_root + fname\n",
        "    out_path = f\"data/downloads/{fname}\"\n",
        "    if not os.path.exists(f\"{out_path}\"):\n",
        "      !wget -O {out_path} {link}\n"
      ],
      "metadata": {
        "id": "87FA0nDmgSMd"
      },
      "id": "87FA0nDmgSMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e38bfcae",
      "metadata": {
        "id": "e38bfcae"
      },
      "outputs": [],
      "source": [
        "# Installs microWakeWord. Be sure to restart the session after this is finished.\n",
        "import platform\n",
        "import sys\n",
        "\n",
        "if platform.system() == \"Darwin\":\n",
        "    # `pymicro-features` is installed from a fork to support building on macOS\n",
        "    !pip install 'git+https://github.com/puddly/pymicro-features@puddly/minimum-cpp-version'\n",
        "\n",
        "# `audio-metadata` is installed from a fork to unpin `attrs` from a version that breaks Jupyter\n",
        "!pip install 'git+https://github.com/whatsnowplaying/audio-metadata@d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f'\n",
        "\n",
        "!git clone -b main https://github.com/kahrendt/microWakeWord\n",
        "\n",
        "# Fix colab compatiblity by replacing some versions in the setup\n",
        "!sed -i -e 's/\">=3.10, <3.11\",/\">=3.10\",/' microWakeWord/setup.py\n",
        "!sed -i -e 's/\"datasets\",/\"datasets>=3,<4\",/' microWakeWord/setup.py\n",
        "!pip install -e ./microWakeWord\n",
        "\n",
        "sys.path.append(\"./microWakeWord/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5142a31",
      "metadata": {
        "id": "f5142a31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if not os.path.exists(\"./piper-sample-generator\"):\n",
        "  !git clone https://github.com/rhasspy/piper-sample-generator\n",
        "  !sed -i -e 's/torch_model = torch.load(model_path)/torch_model = torch.load(model_path, weights_only=False)/' piper-sample-generator/generate_samples.py\n",
        "\n",
        "  !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
        "  !wget -O piper-sample-generator/models/nl_NL-mls-medium.pt https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/nl_NL-mls-medium.pt\n",
        "\n",
        "  !pip install piper-phonemize-cross==1.2.1 torchcodec==0.2.1\n",
        "\n",
        "if \"piper-sample-generator/\" not in sys.path:\n",
        "  sys.path.append(\"./piper-sample-generator/\")\n",
        "  sys.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f41cc620",
      "metadata": {
        "id": "f41cc620"
      },
      "outputs": [],
      "source": [
        "#from generate_samples import generate_samples\n",
        "from microwakeword.audio.augmentation import Augmentation\n",
        "from microwakeword.audio.clips import Clips\n",
        "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
        "from microwakeword.audio.audio_utils import save_clip\n",
        "from mmap_ninja.ragged import RaggedMmap\n",
        "from IPython.display import Audio\n",
        "import datasets\n",
        "import scipy\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b936d0d2",
      "metadata": {
        "id": "b936d0d2"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b830a1",
      "metadata": {
        "id": "64b830a1"
      },
      "outputs": [],
      "source": [
        "WAKEWORD = \"hey_eve,\"\n",
        "SAMPLE_COUNT = 4096\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c15dfa31",
      "metadata": {
        "id": "c15dfa31"
      },
      "source": [
        "## Sample generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e76429a5",
      "metadata": {
        "id": "e76429a5"
      },
      "outputs": [],
      "source": [
        "# Call with python so we get realtime log output\n",
        "!python3 piper-sample-generator/generate_samples.py \"{WAKEWORD}\" \\\n",
        "--max-samples 1 \\\n",
        "--batch-size {BATCH_SIZE} \\\n",
        "--output-dir ./data/samples/en \\\n",
        "--model piper-sample-generator/models/en_US-libritts_r-medium.pt \\\n",
        "--noise-scales 0.333 \\\n",
        "--noise-scale-ws 0.333 \\\n",
        "--min-phoneme-count 80\n",
        "\n",
        "Audio(\"./data/samples/en/0.wav\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call with python so we get realtime log output\n",
        "!python3 piper-sample-generator/generate_samples.py \"{WAKEWORD}\" \\\n",
        "--max-samples 1 \\\n",
        "--batch-size {BATCH_SIZE} \\\n",
        "--output-dir ./data/samples/nl \\\n",
        "--model piper-sample-generator/models/nl_NL-mls-medium.pt\\\n",
        "--noise-scales 0.333 \\\n",
        "--noise-scale-ws 0.333 \\\n",
        "--min-phoneme-count 80\n",
        "\n",
        "Audio(\"./data/samples/nl/0.wav\", autoplay=True)"
      ],
      "metadata": {
        "id": "LJttLhQFwbft"
      },
      "id": "LJttLhQFwbft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b5e532",
      "metadata": {
        "id": "e5b5e532"
      },
      "outputs": [],
      "source": [
        "!python3 piper-sample-generator/generate_samples.py \"{WAKEWORD}\" \\\n",
        "--batch-size {BATCH_SIZE} \\\n",
        "--max-samples {SAMPLE_COUNT} \\\n",
        "--output-dir ./data/samples/en \\\n",
        "--model piper-sample-generator/models/en_US-libritts_r-medium.pt \\\n",
        "--noise-scales 0.333 \\\n",
        "--noise-scale-ws 0.333 \\\n",
        "--min-phoneme-count 80\n",
        "\n",
        "# generate_samples(\n",
        "#     text=WAKEWORD,\n",
        "#     output_dir=\"./data/samples\",\n",
        "#     max_samples=SAMPLE_COUNT,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     model=\"piper-sample-generator/models/en_US-libritts_r-medium.pt\",\n",
        "#     slerp_weights=[0.5],\n",
        "#     length_scales=[1.0, 0.75, 1.25, 1.4],\n",
        "#     noise_scales=[0.333],\n",
        "#     noise_scale_ws=[0.333],\n",
        "#     min_phoneme_count=80\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 piper-sample-generator/generate_samples.py \"{WAKEWORD}\" \\\n",
        "--batch-size {BATCH_SIZE} \\\n",
        "--max-samples {SAMPLE_COUNT} \\\n",
        "--output-dir ./data/samples/nl \\\n",
        "--model piper-sample-generator/models/nl_NL-mls-medium.pt\\\n",
        "--noise-scales 0.333 \\\n",
        "--noise-scale-ws 0.333 \\\n",
        "--min-phoneme-count 80\n",
        "\n",
        "# generate_samples(\n",
        "#     text=WAKEWORD,\n",
        "#     output_dir=\"./data/samples\",\n",
        "#     max_samples=SAMPLE_COUNT,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     model=\"piper-sample-generator/models/en_US-libritts_r-medium.pt\",\n",
        "#     slerp_weights=[0.5],\n",
        "#     length_scales=[1.0, 0.75, 1.25, 1.4],\n",
        "#     noise_scales=[0.333],\n",
        "#     noise_scale_ws=[0.333],\n",
        "#     min_phoneme_count=80\n",
        "# )\n"
      ],
      "metadata": {
        "id": "WWo1Y6CRwYep"
      },
      "id": "WWo1Y6CRwYep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "751d11ad",
      "metadata": {
        "id": "751d11ad"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dff337d",
      "metadata": {
        "id": "7dff337d"
      },
      "source": [
        "### MIT RIR Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c10f400",
      "metadata": {
        "id": "1c10f400"
      },
      "outputs": [],
      "source": [
        "\n",
        "output_dir = \"./data/mit_rirs\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    for row in tqdm(rir_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1]\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96070594",
      "metadata": {
        "id": "96070594"
      },
      "source": [
        "### AudioSet Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712a29e3",
      "metadata": {
        "id": "712a29e3"
      },
      "outputs": [],
      "source": [
        "# AudioSet Dataset (https://research.google.com/audioset/dataset/index.html)\n",
        "\n",
        "if not os.path.exists(\"data/audioset\"):\n",
        "    os.mkdir(\"data/audioset\")\n",
        "\n",
        "    for i in range(10):\n",
        "        fname = f\"bal_train0{i}.tar\"\n",
        "        out_dir = f\"data/downloads/{fname}\"\n",
        "        !tar -xf {out_dir} -C data/audioset\n",
        "\n",
        "# There are a couple of corrupt files in the dataset.\n",
        "!rm -Rf \\\n",
        "  data/audioset/audio/bal_train/d8WgfWSf1VM.flac \\\n",
        "  data/audioset/audio/bal_train/kKf9OprN9nw.flac\n",
        "\n",
        "output_dir = \"./data/audioset_16k\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "# Save clips to 16-bit PCM wav files\n",
        "audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"data/audioset/audio\").glob(\"**/*.flac\")]})\n",
        "audioset_dataset2 = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
        "\n",
        "for row in tqdm(audioset_dataset2):\n",
        "    name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
        "    scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTxMsYim6SV-"
      },
      "id": "xTxMsYim6SV-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2d59cd0b",
      "metadata": {
        "id": "2d59cd0b"
      },
      "source": [
        "### fma_small Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9555cfc1",
      "metadata": {
        "id": "9555cfc1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "output_dir = \"./data/fma_16k\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "# iterate over each subdirectory in the fma_small directory\n",
        "if not os.path.exists(\"./data/fma_small\"):\n",
        "    !unzip ./data/downloads/fma_small.zip -d ./data/fma_small/\n",
        "\n",
        "# More borked files? https://github.com/mdeff/fma/issues/8\n",
        "!rm -f \\\n",
        "  data/fma_small/fma_small/098/098565.mp3 \\\n",
        "  data/fma_small/fma_small/098/098567.mp3 \\\n",
        "  data/fma_small/fma_small/098/098569.mp3 \\\n",
        "  data/fma_small/fma_small/099/099134.mp3 \\\n",
        "  data/fma_small/fma_small/108/108925.mp3 \\\n",
        "  data/fma_small/fma_small/133/133297.mp3\n",
        "\n",
        "# Save clips to 16-bit PCM wav files\n",
        "fma_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for dir in os.listdir(\"data/fma_small\") for i in Path(f\"data/fma_small/{dir}\").glob(\"**/**/*.mp3\")]})\n",
        "fma_dataset2 = fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
        "\n",
        "for row in tqdm(fma_dataset2):\n",
        "    name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
        "    if not os.path.exists(os.path.join(output_dir, name)):\n",
        "        # Convert to 16-bit PCM wav format\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f86382",
      "metadata": {
        "id": "39f86382"
      },
      "source": [
        "### Augmentation\n",
        "Most of this part is from kahrendt/microWakeWord: https://github.com/kahrendt/microWakeWord/blob/main/notebooks/basic_training_notebook.ipynb\n",
        "\n",
        "Only parameters were changed where needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac8ac9e",
      "metadata": {
        "id": "fac8ac9e"
      },
      "outputs": [],
      "source": [
        "# Sets up the augmentations.\n",
        "# To improve your model, experiment with these settings and use more sources of\n",
        "# background clips.\n",
        "\n",
        "en_clips = Clips(input_directory='data/samples/en',\n",
        "              file_pattern='*.wav',\n",
        "              max_clip_duration_s=None,\n",
        "              remove_silence=False,\n",
        "              random_split_seed=10,\n",
        "              split_count=0.1,\n",
        "              )\n",
        "nl_clips = Clips(input_directory='data/samples/nl',\n",
        "              file_pattern='*.wav',\n",
        "              max_clip_duration_s=None,\n",
        "              remove_silence=False,\n",
        "              random_split_seed=10,\n",
        "              split_count=0.1,\n",
        "              )\n",
        "augmenter = Augmentation(augmentation_duration_s=3.2,\n",
        "                         augmentation_probabilities = {\n",
        "                                \"SevenBandParametricEQ\": 0.1,\n",
        "                                \"TanhDistortion\": 0.1,\n",
        "                                \"PitchShift\": 0.1,\n",
        "                                \"BandStopFilter\": 0.1,\n",
        "                                \"AddColorNoise\": 0.1,\n",
        "                                \"AddBackgroundNoise\": 0.75,\n",
        "                                \"Gain\": 1.0,\n",
        "                                \"RIR\": 0.5,\n",
        "                            },\n",
        "                         impulse_paths = ['data/mit_rirs'],\n",
        "                         background_paths = ['data/fma_16k', 'data/audioset_16k'],\n",
        "                         background_min_snr_db = -5,\n",
        "                         background_max_snr_db = 10,\n",
        "                         min_jitter_s = 0.195,\n",
        "                         max_jitter_s = 0.205,\n",
        "                         )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3d2ebe",
      "metadata": {
        "id": "3d3d2ebe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Augment a random clip and play it back to verify it works well\n",
        "\n",
        "random_clip = clips.get_random_clip()\n",
        "augmented_clip = augmenter.augment_clip(random_clip)\n",
        "save_clip(augmented_clip, 'augmented_clip.wav')\n",
        "\n",
        "Audio(\"augmented_clip.wav\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b835d2",
      "metadata": {
        "id": "34b835d2"
      },
      "outputs": [],
      "source": [
        "# Augment samples and save the training, validation, and testing sets.\n",
        "# Validating and testing samples generated the same way can make the model\n",
        "# benchmark better than it performs in real-word use. Use real samples or TTS\n",
        "# samples generated with a different TTS engine to potentially get more accurate\n",
        "# benchmarks.\n",
        "\n",
        "output_dir = 'data/augmented_features'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "splits = [\"training\", \"validation\", \"testing\"]\n",
        "for split in splits:\n",
        "  out_dir = os.path.join(output_dir, split)\n",
        "  if not os.path.exists(out_dir):\n",
        "      os.mkdir(out_dir)\n",
        "\n",
        "\n",
        "  split_name = \"train\"\n",
        "  repetition = 2\n",
        "\n",
        "  spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=10,    # Uses the same spectrogram repeatedly, just shifted over by one frame. This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "  if split == \"validation\":\n",
        "    split_name = \"validation\"\n",
        "    repetition = 1\n",
        "  elif split == \"testing\":\n",
        "    split_name = \"test\"\n",
        "    repetition = 1\n",
        "    spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=1,    # The testing set uses the streaming version of the model, so no artificial repetition is necessary\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "\n",
        "  RaggedMmap.from_generator(\n",
        "      out_dir=os.path.join(out_dir, 'wakeword_mmap'),\n",
        "      sample_generator=spectrograms.spectrogram_generator(split=split_name, repeat=repetition),\n",
        "      batch_size=7,\n",
        "      verbose=True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce074de3",
      "metadata": {
        "id": "ce074de3"
      },
      "source": [
        "### Download Spectogram feautures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72f43ac",
      "metadata": {
        "id": "c72f43ac"
      },
      "outputs": [],
      "source": [
        "# Downloads pre-generated spectrogram features (made for microWakeWord in\n",
        "# particular) for various negative datasets. This can be slow!\n",
        "\n",
        "output_dir = './data/negative_datasets'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
        "    for fname in filenames:\n",
        "        zip_path = f\"data/downloads/{fname}\"\n",
        "        !unzip -q {zip_path} -d {output_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a7e3272",
      "metadata": {
        "id": "4a7e3272"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "129e8e9e",
      "metadata": {
        "id": "129e8e9e"
      },
      "outputs": [],
      "source": [
        "# Save a yaml config that controls the training process\n",
        "# These hyperparamters can make a huge different in model quality.\n",
        "# Experiment with sampling and penalty weights and increasing the number of\n",
        "# training steps.\n",
        "\n",
        "config = {}\n",
        "\n",
        "config[\"window_step_ms\"] = 10\n",
        "\n",
        "config[\"train_dir\"] = (\n",
        "    f\"data/trained_models/{WAKEWORD.replace(',', '')}\"  # Directory where the trained model will be saved\n",
        ")\n",
        "\n",
        "\n",
        "# Each feature_dir should have at least one of the following folders with this structure:\n",
        "#  training/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#\n",
        "#  sampling_weight: Weight for choosing a spectrogram from this set in the batch\n",
        "#  penalty_weight: Penalizing weight for incorrect predictions from this set\n",
        "#  truth: Boolean whether this set has positive samples or negative samples\n",
        "#  truncation_strategy = If spectrograms in the set are longer than necessary for training, how are they truncated\n",
        "#       - random: choose a random portion of the entire spectrogram - useful for long negative samples\n",
        "#       - truncate_start: remove the start of the spectrogram\n",
        "#       - truncate_end: remove the end of the spectrogram\n",
        "#       - split: Split the longer spectrogram into separate spectrograms offset by 100 ms. Only for ambient sets\n",
        "\n",
        "config[\"features\"] = [\n",
        "    {\n",
        "        \"features_dir\": \"data/augmented_features\",\n",
        "        \"sampling_weight\": 2.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": True,\n",
        "        \"truncation_strategy\": \"truncate_start\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"data/negative_datasets/speech\",\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"data/negative_datasets/dinner_party\",\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"data/negative_datasets/no_speech\",\n",
        "        \"sampling_weight\": 5.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    { # Only used for validation and testing\n",
        "        \"features_dir\": \"data/negative_datasets/dinner_party_eval\",\n",
        "        \"sampling_weight\": 0.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"split\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Number of training steps in each iteration - various other settings are configured as lists that corresponds to different steps\n",
        "config[\"training_steps\"] = [100000]  # Number of training steps for each training iteration - list that corresponds to training steps\n",
        "\n",
        "# Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
        "config[\"positive_class_weight\"] = [1]\n",
        "config[\"negative_class_weight\"] = [20]\n",
        "\n",
        "config[\"learning_rates\"] = [\n",
        "    0.001,\n",
        "]  # Learning rates for Adam optimizer - list that corresponds to training steps\n",
        "config[\"batch_size\"] = BATCH_SIZE\n",
        "\n",
        "config[\"time_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"time_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "\n",
        "config[\"eval_step_interval\"] = (\n",
        "    2500  # Test the validation sets after every this many steps\n",
        ")\n",
        "config[\"clip_duration_ms\"] = (\n",
        "    1500  # Maximum length of wake word that the streaming model will accept\n",
        ")\n",
        "\n",
        "# The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
        "# Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
        "# Available metrics:\n",
        "#   - \"loss\" - cross entropy error on validation set\n",
        "#   - \"accuracy\" - accuracy of validation set\n",
        "#   - \"recall\" - recall of validation set\n",
        "#   - \"precision\" - precision of validation set\n",
        "#   - \"false_positive_rate\" - false positive rate of validation set\n",
        "#   - \"false_negative_rate\" - false negative rate of validation set\n",
        "#   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
        "#   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
        "config[\"target_minimization\"] = 0.9\n",
        "config[\"minimization_metric\"] = None  # Set to None to disable\n",
        "\n",
        "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
        "\n",
        "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
        "    documents = yaml.dump(config, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24527804",
      "metadata": {
        "id": "24527804"
      },
      "outputs": [],
      "source": [
        "# Trains a model. When finished, it will quantize and convert the model to a\n",
        "# streaming version suitable for on-device detection.\n",
        "# It will resume if stopped, but it will start over at the configured training\n",
        "# steps in the yaml file.\n",
        "# Change --train 0 to only convert and test the best-weighted model.\n",
        "# On Google colab, it doesn't print the mini-batch results, so it may appear\n",
        "# stuck for several minutes! Additionally, it is very slow compared to training\n",
        "# on a local GPU.\n",
        "\n",
        "!python -m microwakeword.model_train_eval \\\n",
        "--training_config='training_parameters.yaml' \\\n",
        "--train 1 \\\n",
        "--restore_checkpoint 1 \\\n",
        "--test_tf_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming_quantized 0 \\\n",
        "--test_tflite_streaming 0 \\\n",
        "--test_tflite_streaming_quantized 1 \\\n",
        "--use_weights \"best_weights\" \\\n",
        "mixednet \\\n",
        "--pointwise_filters \"64,64,64,64\" \\\n",
        "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
        "--mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
        "--residual_connection \"0,0,0,0\" \\\n",
        "--first_conv_filters 32 \\\n",
        "--first_conv_kernel_size 5 \\\n",
        "--stride 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17768321",
      "metadata": {
        "id": "17768321"
      },
      "source": [
        "## Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb5a8acd",
      "metadata": {
        "id": "bb5a8acd"
      },
      "outputs": [],
      "source": [
        "# Move the model to the models directory\n",
        "os.mkdir(\"./model\")\n",
        "shutil.move(f\"data/trained_models/{WAKEWORD.replace(',', '')}/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\", f\"model/{WAKEWORD.replace(',', '')}.tflite\")\n",
        "shutil.move(f\"data/trained_models/{WAKEWORD.replace(',', '')}/tflite_stream_state_internal_quant/tflite_streaming_roc.txt\", f\"model/{WAKEWORD.replace(',', '')}_roc.txt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}